{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data will go to Bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to minio cline\n",
    "def get_minio_client():\n",
    "    client = Minio(\n",
    "        \"localhost:9000\",\n",
    "        access_key = os.getenv(\"MINIO_USERNAME\"),\n",
    "        secret_key = os.getenv(\"MINIO_PASSWORD\"),\n",
    "        secure = False\n",
    "    )\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(bucket_name, file_path):\n",
    "    try:\n",
    "        client = get_minio_client()\n",
    "\n",
    "        # Check if the bucket exists, if not, create it\n",
    "        if not client.bucket_exists(bucket_name):\n",
    "            print(f\"Bucket '{bucket_name}' does not exist. Creating it now.\")\n",
    "            client.make_bucket(bucket_name)\n",
    "\n",
    "        # Ensure the file path is valid\n",
    "        if not os.path.isfile(file_path):\n",
    "            print(f\"The file path '{file_path}' does not exist or is not a file.\")\n",
    "            return\n",
    "\n",
    "        # Extract the file name from the file path\n",
    "        file_name = os.path.basename(file_path)\n",
    "\n",
    "        # Upload the file to the specified bucket\n",
    "        client.fput_object(bucket_name, file_name, file_path)\n",
    "        print(f\"File '{file_name}' uploaded successfully to bucket '{bucket_name}'.\")\n",
    "\n",
    "    except S3Error as e:\n",
    "        print(f\"S3 Error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Upload raw data to bronze bucket\n",
    "\n",
    "# upload_file(\"bronze\", \"/home/drissdo/Desktop/Scalable-Distributed-Systems/data/Loan_default.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 11:00:54 WARN Utils: Your hostname, dtdat resolves to a loopback address: 127.0.1.1; using 192.168.2.12 instead (on interface wlp0s20f3)\n",
      "24/12/09 11:00:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/12/09 11:00:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, mean\n",
    "\n",
    "# Run full cores\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LoanDefaultPrediction\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\")\\\n",
    "    .config(\"spark.jars\", \"/home/drissdo/Desktop/Scalable-Distributed-Systems/src/jars/aws-java-sdk-bundle-1.11.901.jar, /home/drissdo/Desktop/Scalable-Distributed-Systems/src/jars/hadoop-aws-3.3.1.jar\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 09:34:06 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+\n",
      "|    LoanID|Age|Income|LoanAmount|CreditScore|MonthsEmployed|NumCreditLines|InterestRate|LoanTerm|DTIRatio|  Education|EmploymentType|MaritalStatus|HasMortgage|HasDependents|LoanPurpose|HasCoSigner|Default|\n",
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+\n",
      "|I38PQUQS96| 56| 85994|     50587|        520|            80|             4|       15.23|      36|    0.44| Bachelor's|     Full-time|     Divorced|        Yes|          Yes|      Other|        Yes|      0|\n",
      "|HPSK72WA7R| 69| 50432|    124440|        458|            15|             1|        4.81|      60|    0.68|   Master's|     Full-time|      Married|         No|           No|      Other|        Yes|      0|\n",
      "|C1OZ6DPJ8Y| 46| 84208|    129188|        451|            26|             3|       21.17|      24|    0.31|   Master's|    Unemployed|     Divorced|        Yes|          Yes|       Auto|         No|      1|\n",
      "|V2KKSFM3UN| 32| 31713|     44799|        743|             0|             3|        7.07|      24|    0.23|High School|     Full-time|      Married|         No|           No|   Business|         No|      0|\n",
      "|EY08JDHTZP| 60| 20437|      9139|        633|             8|             4|        6.51|      48|    0.73| Bachelor's|    Unemployed|     Divorced|         No|          Yes|       Auto|         No|      0|\n",
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = \"s3a://bronze/Loan_default.csv\"\n",
    "\n",
    "# Read the CSV file from S3\n",
    "loan_data = spark.read.csv(data_path, inferSchema=True, header=True)\n",
    "\n",
    "# Display the first 5 rows\n",
    "loan_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LoanID',\n",
       " 'Age',\n",
       " 'Income',\n",
       " 'LoanAmount',\n",
       " 'CreditScore',\n",
       " 'MonthsEmployed',\n",
       " 'NumCreditLines',\n",
       " 'InterestRate',\n",
       " 'LoanTerm',\n",
       " 'DTIRatio',\n",
       " 'Education',\n",
       " 'EmploymentType',\n",
       " 'MaritalStatus',\n",
       " 'HasMortgage',\n",
       " 'HasDependents',\n",
       " 'LoanPurpose',\n",
       " 'HasCoSigner',\n",
       " 'Default']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LoanID: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      " |-- LoanAmount: integer (nullable = true)\n",
      " |-- CreditScore: integer (nullable = true)\n",
      " |-- MonthsEmployed: integer (nullable = true)\n",
      " |-- NumCreditLines: integer (nullable = true)\n",
      " |-- InterestRate: double (nullable = true)\n",
      " |-- LoanTerm: integer (nullable = true)\n",
      " |-- DTIRatio: double (nullable = true)\n",
      " |-- Education: string (nullable = true)\n",
      " |-- EmploymentType: string (nullable = true)\n",
      " |-- MaritalStatus: string (nullable = true)\n",
      " |-- HasMortgage: string (nullable = true)\n",
      " |-- HasDependents: string (nullable = true)\n",
      " |-- LoanPurpose: string (nullable = true)\n",
      " |-- HasCoSigner: string (nullable = true)\n",
      " |-- Default: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loan_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# Handle missing values\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"Income\", \"MonthsEmployed\", \"NumCreditLines\", \"InterestRate\", \"LoanTerm\", \"DTIRatio\"],\n",
    "    outputCols=[\"Income_filled\", \"MonthsEmployed_filled\", \"NumCreditLines_filled\", \"InterestRate_filled\", \"LoanTerm_filled\", \"DTIRatio_filled\"]\n",
    ")\n",
    "loan_data_imputed = imputer.fit(loan_data).transform(loan_data)\n",
    "\n",
    "string_indexers = [\n",
    "    StringIndexer(inputCol=\"Education\", outputCol=\"Education_index\"),\n",
    "    StringIndexer(inputCol=\"EmploymentType\", outputCol=\"EmploymentType_index\"),\n",
    "    StringIndexer(inputCol=\"MaritalStatus\", outputCol=\"MaritalStatus_index\"),\n",
    "    StringIndexer(inputCol=\"HasMortgage\", outputCol=\"HasMortgage_index\"),\n",
    "    StringIndexer(inputCol=\"HasDependents\", outputCol=\"HasDependents_index\"),\n",
    "    StringIndexer(inputCol=\"LoanPurpose\", outputCol=\"LoanPurpose_index\"),\n",
    "    StringIndexer(inputCol=\"HasCoSigner\", outputCol=\"HasCoSigner_index\")\n",
    "]\n",
    "\n",
    "pipeline_indexers = Pipeline(stages=string_indexers)\n",
    "loan_data_indexed = pipeline_indexers.fit(loan_data_imputed).transform(loan_data_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# One-hot encode indexed columns\n",
    "one_hot_encoders = [\n",
    "    OneHotEncoder(inputCol=\"Education_index\", outputCol=\"Education_vec\"),\n",
    "    OneHotEncoder(inputCol=\"EmploymentType_index\", outputCol=\"EmploymentType_vec\"),\n",
    "    OneHotEncoder(inputCol=\"MaritalStatus_index\", outputCol=\"MaritalStatus_vec\"),\n",
    "    OneHotEncoder(inputCol=\"HasMortgage_index\", outputCol=\"HasMortgage_vec\"),\n",
    "    OneHotEncoder(inputCol=\"HasDependents_index\", outputCol=\"HasDependents_vec\"),\n",
    "    OneHotEncoder(inputCol=\"LoanPurpose_index\", outputCol=\"LoanPurpose_vec\"),\n",
    "    OneHotEncoder(inputCol=\"HasCoSigner_index\", outputCol=\"HasCoSigner_vec\")\n",
    "]\n",
    "\n",
    "pipeline_encoders = Pipeline(stages=one_hot_encoders)\n",
    "loan_data_encoded = pipeline_encoders.fit(loan_data_indexed).transform(loan_data_indexed)\n",
    "\n",
    "# Normalize numerical features\n",
    "numerical_cols = [\n",
    "    \"Age\", \"Income_filled\", \"LoanAmount\", \"CreditScore\", \"MonthsEmployed_filled\",\n",
    "    \"NumCreditLines_filled\", \"InterestRate_filled\", \"LoanTerm_filled\", \"DTIRatio_filled\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"features\")\n",
    "loan_data_assembled = assembler.transform(loan_data_encoded)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(loan_data_assembled)\n",
    "loan_data_scaled = scaler_model.transform(loan_data_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 09:34:15 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+-------------+---------------------+---------------------+-------------------+---------------+---------------+---------------+--------------------+-------------------+-----------------+-------------------+-----------------+-----------------+-------------+------------------+-----------------+---------------+-----------------+---------------+---------------+--------------------+--------------------+\n",
      "|    LoanID|Age|Income|LoanAmount|CreditScore|MonthsEmployed|NumCreditLines|InterestRate|LoanTerm|DTIRatio|  Education|EmploymentType|MaritalStatus|HasMortgage|HasDependents|LoanPurpose|HasCoSigner|Default|Income_filled|MonthsEmployed_filled|NumCreditLines_filled|InterestRate_filled|LoanTerm_filled|DTIRatio_filled|Education_index|EmploymentType_index|MaritalStatus_index|HasMortgage_index|HasDependents_index|LoanPurpose_index|HasCoSigner_index|Education_vec|EmploymentType_vec|MaritalStatus_vec|HasMortgage_vec|HasDependents_vec|LoanPurpose_vec|HasCoSigner_vec|            features|     scaled_features|\n",
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+-------------+---------------------+---------------------+-------------------+---------------+---------------+---------------+--------------------+-------------------+-----------------+-------------------+-----------------+-----------------+-------------+------------------+-----------------+---------------+-----------------+---------------+---------------+--------------------+--------------------+\n",
      "|I38PQUQS96| 56| 85994|     50587|        520|            80|             4|       15.23|      36|    0.44| Bachelor's|     Full-time|     Divorced|        Yes|          Yes|      Other|        Yes|      0|        85994|                   80|                    4|              15.23|             36|           0.44|            0.0|                 3.0|                1.0|              0.0|                0.0|              3.0|              0.0|(3,[0],[1.0])|         (3,[],[])|    (2,[1],[1.0])|  (1,[0],[1.0])|    (1,[0],[1.0])|  (4,[3],[1.0])|  (1,[0],[1.0])|[56.0,85994.0,505...|[0.83398787561710...|\n",
      "|HPSK72WA7R| 69| 50432|    124440|        458|            15|             1|        4.81|      60|    0.68|   Master's|     Full-time|      Married|         No|           No|      Other|        Yes|      0|        50432|                   15|                    1|               4.81|             60|           0.68|            2.0|                 3.0|                0.0|              1.0|                1.0|              3.0|              0.0|(3,[2],[1.0])|         (3,[],[])|    (2,[0],[1.0])|      (1,[],[])|        (1,[],[])|  (4,[3],[1.0])|  (1,[0],[1.0])|[69.0,50432.0,124...|[1.70121775497492...|\n",
      "|C1OZ6DPJ8Y| 46| 84208|    129188|        451|            26|             3|       21.17|      24|    0.31|   Master's|    Unemployed|     Divorced|        Yes|          Yes|       Auto|         No|      1|        84208|                   26|                    3|              21.17|             24|           0.31|            2.0|                 1.0|                1.0|              0.0|                0.0|              4.0|              1.0|(3,[2],[1.0])|     (3,[1],[1.0])|    (2,[1],[1.0])|  (1,[0],[1.0])|    (1,[0],[1.0])|      (4,[],[])|      (1,[],[])|[46.0,84208.0,129...|[0.16688796841878...|\n",
      "|V2KKSFM3UN| 32| 31713|     44799|        743|             0|             3|        7.07|      24|    0.23|High School|     Full-time|      Married|         No|           No|   Business|         No|      0|        31713|                    0|                    3|               7.07|             24|           0.23|            1.0|                 3.0|                0.0|              1.0|                1.0|              0.0|              1.0|(3,[1],[1.0])|         (3,[],[])|    (2,[0],[1.0])|      (1,[],[])|        (1,[],[])|  (4,[0],[1.0])|      (1,[],[])|[32.0,31713.0,447...|[-0.7670519016588...|\n",
      "|EY08JDHTZP| 60| 20437|      9139|        633|             8|             4|        6.51|      48|    0.73| Bachelor's|    Unemployed|     Divorced|         No|          Yes|       Auto|         No|      0|        20437|                    8|                    4|               6.51|             48|           0.73|            0.0|                 1.0|                1.0|              1.0|                0.0|              4.0|              1.0|(3,[0],[1.0])|     (3,[1],[1.0])|    (2,[1],[1.0])|      (1,[],[])|    (1,[0],[1.0])|      (4,[],[])|      (1,[],[])|[60.0,20437.0,913...|[1.10082783849643...|\n",
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+-------------+---------------------+---------------------+-------------------+---------------+---------------+---------------+--------------------+-------------------+-----------------+-------------------+-----------------+-----------------+-------------+------------------+-----------------+---------------+-----------------+---------------+---------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loan_data_scaled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save data to sliver bucket\n",
    "silver_bucket_path = \"s3a://sliver/preprocessed_loan_data\"\n",
    "loan_data_scaled.repartition(1).write.parquet(silver_bucket_path,  mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, rand\n",
    "\n",
    "\n",
    "loan_data_scaled = spark.read.parquet(silver_bucket_path)\n",
    "\n",
    "# Ensure the split maintains the balance of defaulted and non-defaulted loans\n",
    "default_data = loan_data_scaled.filter(col(\"Default\") == 1)\n",
    "non_default_data = loan_data_scaled.filter(col(\"Default\") == 0)\n",
    "\n",
    "train_default, test_default = default_data.randomSplit([0.8, 0.2], seed=42)\n",
    "train_non_default, test_non_default = non_default_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_data = train_default.union(train_non_default)\n",
    "test_data = test_default.union(test_non_default)\n",
    "\n",
    "# Shuffle the data\n",
    "train_data = train_data.orderBy(rand(seed=42))\n",
    "test_data = test_data.orderBy(rand(seed=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - ROC AUC: 0.7336526258585199\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"Default\", maxIter=10)\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Default\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "roc_auc_lr = evaluator.evaluate(lr_predictions)\n",
    "print(f\"Logistic Regression - ROC AUC: {roc_auc_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - ROC AUC: 0.3557278381793549\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol=\"scaled_features\", labelCol=\"Default\", maxDepth=10)\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_dt = evaluator.evaluate(dt_predictions)\n",
    "print(f\"Decision Tree - ROC AUC: {roc_auc_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 227:============================>                           (6 + 6) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - ROC AUC: 0.7150246405034607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=\"Default\", numTrees=100, maxDepth=5)\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_rf = evaluator.evaluate(rf_predictions)\n",
    "print(f\"Random Forest - ROC AUC: {roc_auc_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient-Boosted Trees - ROC AUC: 0.7355670639833097\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(featuresCol=\"scaled_features\", labelCol=\"Default\", maxIter=10, maxDepth=5)\n",
    "gbt_model = gbt.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_gbt = evaluator.evaluate(gbt_predictions)\n",
    "print(f\"Gradient-Boosted Trees - ROC AUC: {roc_auc_gbt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy: 0.8869761951603384, Precision: 0.8566619835968656, Recall: 0.8869761951603384, F1 Score: 0.8379622945462244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Accuracy: 0.8847530985638402, Precision: 0.8417225071170463, Recall: 0.8847530985638402, F1 Score: 0.8433472754461214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Accuracy: 0.885992524099941, Precision: 0.7849827527609845, Recall: 0.885992524099941, F1 Score: 0.8324346387699544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 497:=====================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient-Boosted Trees - Accuracy: 0.8878418256934881, Precision: 0.8600666981625716, Recall: 0.887841825693488, F1 Score: 0.8415473055394449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Define evaluators\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"Default\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"Default\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"Default\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"Default\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "accuracy_lr = accuracy_evaluator.evaluate(lr_predictions)\n",
    "precision_lr = precision_evaluator.evaluate(lr_predictions)\n",
    "recall_lr = recall_evaluator.evaluate(lr_predictions)\n",
    "f1_lr = f1_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"Logistic Regression - Accuracy: {accuracy_lr}, Precision: {precision_lr}, Recall: {recall_lr}, F1 Score: {f1_lr}\")\n",
    "\n",
    "# Evaluate Decision Tree\n",
    "accuracy_dt = accuracy_evaluator.evaluate(dt_predictions)\n",
    "precision_dt = precision_evaluator.evaluate(dt_predictions)\n",
    "recall_dt = recall_evaluator.evaluate(dt_predictions)\n",
    "f1_dt = f1_evaluator.evaluate(dt_predictions)\n",
    "\n",
    "print(f\"Decision Tree - Accuracy: {accuracy_dt}, Precision: {precision_dt}, Recall: {recall_dt}, F1 Score: {f1_dt}\")\n",
    "\n",
    "# Evaluate Random Forest\n",
    "accuracy_rf = accuracy_evaluator.evaluate(rf_predictions)\n",
    "precision_rf = precision_evaluator.evaluate(rf_predictions)\n",
    "recall_rf = recall_evaluator.evaluate(rf_predictions)\n",
    "f1_rf = f1_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Random Forest - Accuracy: {accuracy_rf}, Precision: {precision_rf}, Recall: {recall_rf}, F1 Score: {f1_rf}\")\n",
    "\n",
    "# Evaluate Gradient-Boosted Trees\n",
    "accuracy_gbt = accuracy_evaluator.evaluate(gbt_predictions)\n",
    "precision_gbt = precision_evaluator.evaluate(gbt_predictions)\n",
    "recall_gbt = recall_evaluator.evaluate(gbt_predictions)\n",
    "f1_gbt = f1_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "print(f\"Gradient-Boosted Trees - Accuracy: {accuracy_gbt}, Precision: {precision_gbt}, Recall: {recall_gbt}, F1 Score: {f1_gbt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validated Model - ROC AUC: 0.7380937269287152\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Define parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(dt.maxDepth, [3, 5]) \\\n",
    "    .addGrid(rf.numTrees, [50, 100]) \\\n",
    "    .addGrid(gbt.maxIter, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "# Define cross-validator\n",
    "crossval = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Fit the model\n",
    "cvModel = crossval.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "cv_predictions = cvModel.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_cv = evaluator.evaluate(cv_predictions)\n",
    "print(f\"Cross-Validated Model - ROC AUC: {roc_auc_cv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
