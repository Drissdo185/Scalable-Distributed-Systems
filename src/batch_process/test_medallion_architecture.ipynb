{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data will go to Bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to minio cline\n",
    "def get_minio_client():\n",
    "    client = Minio(\n",
    "        \"localhost:9000\",\n",
    "        access_key = os.getenv(\"MINIO_USERNAME\"),\n",
    "        secret_key = os.getenv(\"MINIO_PASSWORD\"),\n",
    "        secure = False\n",
    "    )\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(bucket_name, file_path):\n",
    "    try:\n",
    "        client = get_minio_client()\n",
    "\n",
    "        # Check if the bucket exists, if not, create it\n",
    "        if not client.bucket_exists(bucket_name):\n",
    "            print(f\"Bucket '{bucket_name}' does not exist. Creating it now.\")\n",
    "            client.make_bucket(bucket_name)\n",
    "\n",
    "        # Ensure the file path is valid\n",
    "        if not os.path.isfile(file_path):\n",
    "            print(f\"The file path '{file_path}' does not exist or is not a file.\")\n",
    "            return\n",
    "\n",
    "        # Extract the file name from the file path\n",
    "        file_name = os.path.basename(file_path)\n",
    "\n",
    "        # Upload the file to the specified bucket\n",
    "        client.fput_object(bucket_name, file_name, file_path)\n",
    "        print(f\"File '{file_name}' uploaded successfully to bucket '{bucket_name}'.\")\n",
    "\n",
    "    except S3Error as e:\n",
    "        print(f\"S3 Error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload raw data to bronze bucket\n",
    "\n",
    "# upload_file(\"bronze\", \"/home/drissdo/Desktop/Scalable-Distributed-Systems/data/Loan_default.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/11 14:14:32 WARN Utils: Your hostname, dtdat resolves to a loopback address: 127.0.1.1; using 192.168.2.12 instead (on interface wlp0s20f3)\n",
      "24/12/11 14:14:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/12/11 14:14:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, mean\n",
    "\n",
    "# Run full cores\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LoanDefaultPrediction\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\")\\\n",
    "    .config(\"spark.jars\", \"/home/drissdo/Desktop/Scalable-Distributed-Systems/src/jars/aws-java-sdk-bundle-1.11.901.jar, /home/drissdo/Desktop/Scalable-Distributed-Systems/src/jars/hadoop-aws-3.3.1.jar\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/11 14:14:35 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+\n",
      "|    LoanID|Age|Income|LoanAmount|CreditScore|MonthsEmployed|NumCreditLines|InterestRate|LoanTerm|DTIRatio|  Education|EmploymentType|MaritalStatus|HasMortgage|HasDependents|LoanPurpose|HasCoSigner|Default|\n",
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+\n",
      "|I38PQUQS96| 56| 85994|     50587|        520|            80|             4|       15.23|      36|    0.44| Bachelor's|     Full-time|     Divorced|        Yes|          Yes|      Other|        Yes|      0|\n",
      "|HPSK72WA7R| 69| 50432|    124440|        458|            15|             1|        4.81|      60|    0.68|   Master's|     Full-time|      Married|         No|           No|      Other|        Yes|      0|\n",
      "|C1OZ6DPJ8Y| 46| 84208|    129188|        451|            26|             3|       21.17|      24|    0.31|   Master's|    Unemployed|     Divorced|        Yes|          Yes|       Auto|         No|      1|\n",
      "|V2KKSFM3UN| 32| 31713|     44799|        743|             0|             3|        7.07|      24|    0.23|High School|     Full-time|      Married|         No|           No|   Business|         No|      0|\n",
      "|EY08JDHTZP| 60| 20437|      9139|        633|             8|             4|        6.51|      48|    0.73| Bachelor's|    Unemployed|     Divorced|         No|          Yes|       Auto|         No|      0|\n",
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = \"s3a://bronze/Loan_default.csv\"\n",
    "\n",
    "# Read the CSV file from S3\n",
    "loan_data = spark.read.csv(data_path, inferSchema=True, header=True)\n",
    "\n",
    "# Display the first 5 rows\n",
    "loan_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LoanID',\n",
       " 'Age',\n",
       " 'Income',\n",
       " 'LoanAmount',\n",
       " 'CreditScore',\n",
       " 'MonthsEmployed',\n",
       " 'NumCreditLines',\n",
       " 'InterestRate',\n",
       " 'LoanTerm',\n",
       " 'DTIRatio',\n",
       " 'Education',\n",
       " 'EmploymentType',\n",
       " 'MaritalStatus',\n",
       " 'HasMortgage',\n",
       " 'HasDependents',\n",
       " 'LoanPurpose',\n",
       " 'HasCoSigner',\n",
       " 'Default']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LoanID: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      " |-- LoanAmount: integer (nullable = true)\n",
      " |-- CreditScore: integer (nullable = true)\n",
      " |-- MonthsEmployed: integer (nullable = true)\n",
      " |-- NumCreditLines: integer (nullable = true)\n",
      " |-- InterestRate: double (nullable = true)\n",
      " |-- LoanTerm: integer (nullable = true)\n",
      " |-- DTIRatio: double (nullable = true)\n",
      " |-- Education: string (nullable = true)\n",
      " |-- EmploymentType: string (nullable = true)\n",
      " |-- MaritalStatus: string (nullable = true)\n",
      " |-- HasMortgage: string (nullable = true)\n",
      " |-- HasDependents: string (nullable = true)\n",
      " |-- LoanPurpose: string (nullable = true)\n",
      " |-- HasCoSigner: string (nullable = true)\n",
      " |-- Default: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/11 14:14:49 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "loan_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"Income\", \"MonthsEmployed\", \"NumCreditLines\", \"InterestRate\", \"LoanTerm\", \"DTIRatio\"],\n",
    "    outputCols=[\"Income_filled\", \"MonthsEmployed_filled\", \"NumCreditLines_filled\", \"InterestRate_filled\", \"LoanTerm_filled\", \"DTIRatio_filled\"]\n",
    ")\n",
    "loan_data_imputed = imputer.fit(loan_data).transform(loan_data)\n",
    "\n",
    "string_indexers = [\n",
    "    StringIndexer(inputCol=\"Education\", outputCol=\"Education_index\"),\n",
    "    StringIndexer(inputCol=\"EmploymentType\", outputCol=\"EmploymentType_index\"),\n",
    "    StringIndexer(inputCol=\"MaritalStatus\", outputCol=\"MaritalStatus_index\"),\n",
    "    StringIndexer(inputCol=\"HasMortgage\", outputCol=\"HasMortgage_index\"),\n",
    "    StringIndexer(inputCol=\"HasDependents\", outputCol=\"HasDependents_index\"),\n",
    "    StringIndexer(inputCol=\"LoanPurpose\", outputCol=\"LoanPurpose_index\"),\n",
    "    StringIndexer(inputCol=\"HasCoSigner\", outputCol=\"HasCoSigner_index\")\n",
    "]\n",
    "\n",
    "pipeline_indexers = Pipeline(stages=string_indexers)\n",
    "loan_data_indexed = pipeline_indexers.fit(loan_data_imputed).transform(loan_data_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# One-hot encode indexed columns\n",
    "one_hot_encoders = [\n",
    "    OneHotEncoder(inputCol=\"Education_index\", outputCol=\"Education_vec\"),\n",
    "    OneHotEncoder(inputCol=\"EmploymentType_index\", outputCol=\"EmploymentType_vec\"),\n",
    "    OneHotEncoder(inputCol=\"MaritalStatus_index\", outputCol=\"MaritalStatus_vec\"),\n",
    "    OneHotEncoder(inputCol=\"HasMortgage_index\", outputCol=\"HasMortgage_vec\"),\n",
    "    OneHotEncoder(inputCol=\"HasDependents_index\", outputCol=\"HasDependents_vec\"),\n",
    "    OneHotEncoder(inputCol=\"LoanPurpose_index\", outputCol=\"LoanPurpose_vec\"),\n",
    "    OneHotEncoder(inputCol=\"HasCoSigner_index\", outputCol=\"HasCoSigner_vec\")\n",
    "]\n",
    "\n",
    "pipeline_encoders = Pipeline(stages=one_hot_encoders)\n",
    "loan_data_encoded = pipeline_encoders.fit(loan_data_indexed).transform(loan_data_indexed)\n",
    "\n",
    "# Normalize numerical features\n",
    "numerical_cols = [\n",
    "    \"Age\", \"Income_filled\", \"LoanAmount\", \"CreditScore\", \"MonthsEmployed_filled\",\n",
    "    \"NumCreditLines_filled\", \"InterestRate_filled\", \"LoanTerm_filled\", \"DTIRatio_filled\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"features\")\n",
    "loan_data_assembled = assembler.transform(loan_data_encoded)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(loan_data_assembled)\n",
    "loan_data_scaled = scaler_model.transform(loan_data_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/11 14:15:11 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+-------------+---------------------+---------------------+-------------------+---------------+---------------+---------------+--------------------+-------------------+-----------------+-------------------+-----------------+-----------------+-------------+------------------+-----------------+---------------+-----------------+---------------+---------------+--------------------+--------------------+\n",
      "|    LoanID|Age|Income|LoanAmount|CreditScore|MonthsEmployed|NumCreditLines|InterestRate|LoanTerm|DTIRatio|  Education|EmploymentType|MaritalStatus|HasMortgage|HasDependents|LoanPurpose|HasCoSigner|Default|Income_filled|MonthsEmployed_filled|NumCreditLines_filled|InterestRate_filled|LoanTerm_filled|DTIRatio_filled|Education_index|EmploymentType_index|MaritalStatus_index|HasMortgage_index|HasDependents_index|LoanPurpose_index|HasCoSigner_index|Education_vec|EmploymentType_vec|MaritalStatus_vec|HasMortgage_vec|HasDependents_vec|LoanPurpose_vec|HasCoSigner_vec|            features|     scaled_features|\n",
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+-------------+---------------------+---------------------+-------------------+---------------+---------------+---------------+--------------------+-------------------+-----------------+-------------------+-----------------+-----------------+-------------+------------------+-----------------+---------------+-----------------+---------------+---------------+--------------------+--------------------+\n",
      "|I38PQUQS96| 56| 85994|     50587|        520|            80|             4|       15.23|      36|    0.44| Bachelor's|     Full-time|     Divorced|        Yes|          Yes|      Other|        Yes|      0|        85994|                   80|                    4|              15.23|             36|           0.44|            0.0|                 3.0|                1.0|              0.0|                0.0|              3.0|              0.0|(3,[0],[1.0])|         (3,[],[])|    (2,[1],[1.0])|  (1,[0],[1.0])|    (1,[0],[1.0])|  (4,[3],[1.0])|  (1,[0],[1.0])|[56.0,85994.0,505...|[0.83398787561710...|\n",
      "|HPSK72WA7R| 69| 50432|    124440|        458|            15|             1|        4.81|      60|    0.68|   Master's|     Full-time|      Married|         No|           No|      Other|        Yes|      0|        50432|                   15|                    1|               4.81|             60|           0.68|            2.0|                 3.0|                0.0|              1.0|                1.0|              3.0|              0.0|(3,[2],[1.0])|         (3,[],[])|    (2,[0],[1.0])|      (1,[],[])|        (1,[],[])|  (4,[3],[1.0])|  (1,[0],[1.0])|[69.0,50432.0,124...|[1.70121775497492...|\n",
      "|C1OZ6DPJ8Y| 46| 84208|    129188|        451|            26|             3|       21.17|      24|    0.31|   Master's|    Unemployed|     Divorced|        Yes|          Yes|       Auto|         No|      1|        84208|                   26|                    3|              21.17|             24|           0.31|            2.0|                 1.0|                1.0|              0.0|                0.0|              4.0|              1.0|(3,[2],[1.0])|     (3,[1],[1.0])|    (2,[1],[1.0])|  (1,[0],[1.0])|    (1,[0],[1.0])|      (4,[],[])|      (1,[],[])|[46.0,84208.0,129...|[0.16688796841878...|\n",
      "|V2KKSFM3UN| 32| 31713|     44799|        743|             0|             3|        7.07|      24|    0.23|High School|     Full-time|      Married|         No|           No|   Business|         No|      0|        31713|                    0|                    3|               7.07|             24|           0.23|            1.0|                 3.0|                0.0|              1.0|                1.0|              0.0|              1.0|(3,[1],[1.0])|         (3,[],[])|    (2,[0],[1.0])|      (1,[],[])|        (1,[],[])|  (4,[0],[1.0])|      (1,[],[])|[32.0,31713.0,447...|[-0.7670519016588...|\n",
      "|EY08JDHTZP| 60| 20437|      9139|        633|             8|             4|        6.51|      48|    0.73| Bachelor's|    Unemployed|     Divorced|         No|          Yes|       Auto|         No|      0|        20437|                    8|                    4|               6.51|             48|           0.73|            0.0|                 1.0|                1.0|              1.0|                0.0|              4.0|              1.0|(3,[0],[1.0])|     (3,[1],[1.0])|    (2,[1],[1.0])|      (1,[],[])|    (1,[0],[1.0])|      (4,[],[])|      (1,[],[])|[60.0,20437.0,913...|[1.10082783849643...|\n",
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+-------------+---------------------+---------------------+-------------------+---------------+---------------+---------------+--------------------+-------------------+-----------------+-------------------+-----------------+-----------------+-------------+------------------+-----------------+---------------+-----------------+---------------+---------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loan_data_scaled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save data to sliver bucket\n",
    "silver_bucket_path = \"s3a://sliver/preprocessed_loan_data\"\n",
    "loan_data_scaled.repartition(1).write.parquet(silver_bucket_path,  mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, rand\n",
    "\n",
    "\n",
    "loan_data_scaled = spark.read.parquet(silver_bucket_path)\n",
    "\n",
    "# Ensure the split maintains the balance of defaulted and non-defaulted loans\n",
    "default_data = loan_data_scaled.filter(col(\"Default\") == 1)\n",
    "non_default_data = loan_data_scaled.filter(col(\"Default\") == 0)\n",
    "\n",
    "train_default, test_default = default_data.randomSplit([0.8, 0.2], seed=42)\n",
    "train_non_default, test_non_default = non_default_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_data = train_default.union(train_non_default)\n",
    "test_data = test_default.union(test_non_default)\n",
    "\n",
    "# Shuffle the data\n",
    "train_data = train_data.orderBy(rand(seed=42))\n",
    "test_data = test_data.orderBy(rand(seed=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:====================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+-------------+---------------------+---------------------+-------------------+---------------+---------------+---------------+--------------------+-------------------+-----------------+-------------------+-----------------+-----------------+-------------+------------------+-----------------+---------------+-----------------+---------------+---------------+--------------------+--------------------+\n",
      "|    LoanID|Age|Income|LoanAmount|CreditScore|MonthsEmployed|NumCreditLines|InterestRate|LoanTerm|DTIRatio|  Education|EmploymentType|MaritalStatus|HasMortgage|HasDependents|LoanPurpose|HasCoSigner|Default|Income_filled|MonthsEmployed_filled|NumCreditLines_filled|InterestRate_filled|LoanTerm_filled|DTIRatio_filled|Education_index|EmploymentType_index|MaritalStatus_index|HasMortgage_index|HasDependents_index|LoanPurpose_index|HasCoSigner_index|Education_vec|EmploymentType_vec|MaritalStatus_vec|HasMortgage_vec|HasDependents_vec|LoanPurpose_vec|HasCoSigner_vec|            features|     scaled_features|\n",
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+-------------+---------------------+---------------------+-------------------+---------------+---------------+---------------+--------------------+-------------------+-----------------+-------------------+-----------------+-----------------+-------------+------------------+-----------------+---------------+-----------------+---------------+---------------+--------------------+--------------------+\n",
      "|7YG865MCWP| 40| 25706|    218715|        722|            65|             4|       23.31|      36|    0.36| Bachelor's|     Full-time|     Divorced|        Yes|          Yes|   Business|         No|      0|        25706|                   65|                    4|              23.31|             36|           0.36|            0.0|                 3.0|                1.0|              0.0|                0.0|              0.0|              1.0|(3,[0],[1.0])|         (3,[],[])|    (2,[1],[1.0])|  (1,[0],[1.0])|    (1,[0],[1.0])|  (4,[0],[1.0])|      (1,[],[])|[40.0,25706.0,218...|[-0.2333719759002...|\n",
      "|BEHFNNEUT1| 26| 95203|    168039|        500|             7|             4|       14.38|      60|    0.12|   Master's|     Full-time|       Single|        Yes|           No|   Business|         No|      1|        95203|                    7|                    4|              14.38|             60|           0.12|            2.0|                 3.0|                2.0|              0.0|                1.0|              0.0|              1.0|(3,[2],[1.0])|         (3,[],[])|        (2,[],[])|  (1,[0],[1.0])|        (1,[],[])|  (4,[0],[1.0])|      (1,[],[])|[26.0,95203.0,168...|[-1.1673118459778...|\n",
      "|T6RFSXUUDJ| 53|108351|    119754|        485|            14|             2|       21.19|      12|    0.21| Bachelor's|     Part-time|       Single|        Yes|          Yes|       Home|         No|      0|       108351|                   14|                    2|              21.19|             12|           0.21|            0.0|                 0.0|                2.0|              0.0|                0.0|              1.0|              1.0|(3,[0],[1.0])|     (3,[0],[1.0])|        (2,[],[])|  (1,[0],[1.0])|    (1,[0],[1.0])|  (4,[1],[1.0])|      (1,[],[])|[53.0,108351.0,11...|[0.63385790345760...|\n",
      "|J2BKQP9IU7| 69| 87222|    136464|        636|           116|             1|        6.66|      12|    0.38|        PhD|     Part-time|       Single|         No|          Yes|  Education|        Yes|      0|        87222|                  116|                    1|               6.66|             12|           0.38|            3.0|                 0.0|                2.0|              1.0|                0.0|              2.0|              0.0|    (3,[],[])|     (3,[0],[1.0])|        (2,[],[])|      (1,[],[])|    (1,[0],[1.0])|  (4,[2],[1.0])|  (1,[0],[1.0])|[69.0,87222.0,136...|[1.70121775497492...|\n",
      "|XHZW2365P0| 22|141044|    117053|        446|            43|             2|        3.61|      12|    0.65|   Master's| Self-employed|      Married|         No|           No|       Auto|         No|      0|       141044|                   43|                    2|               3.61|             12|           0.65|            2.0|                 2.0|                0.0|              1.0|                1.0|              4.0|              1.0|(3,[2],[1.0])|     (3,[2],[1.0])|    (2,[0],[1.0])|      (1,[],[])|        (1,[],[])|      (4,[],[])|      (1,[],[])|[22.0,141044.0,11...|[-1.4341518088571...|\n",
      "|AYDNIXAW5E| 55|105366|     30862|        611|            69|             2|       11.89|      60|    0.66|High School|    Unemployed|       Single|        Yes|           No|       Home|        Yes|      0|       105366|                   69|                    2|              11.89|             60|           0.66|            1.0|                 1.0|                2.0|              0.0|                1.0|              1.0|              0.0|(3,[1],[1.0])|     (3,[1],[1.0])|        (2,[],[])|  (1,[0],[1.0])|        (1,[],[])|  (4,[1],[1.0])|  (1,[0],[1.0])|[55.0,105366.0,30...|[0.76727788489727...|\n",
      "|H2BOVDFE2S| 66|140900|    199339|        545|           108|             2|       14.65|      12|    0.35|        PhD| Self-employed|       Single|         No|           No|  Education|         No|      0|       140900|                  108|                    2|              14.65|             12|           0.35|            3.0|                 2.0|                2.0|              1.0|                1.0|              2.0|              1.0|    (3,[],[])|     (3,[2],[1.0])|        (2,[],[])|      (1,[],[])|        (1,[],[])|  (4,[2],[1.0])|      (1,[],[])|[66.0,140900.0,19...|[1.50108778281542...|\n",
      "|V4138GAD7D| 35|121173|    209336|        495|             0|             1|       23.17|      60|    0.67|High School|    Unemployed|      Married|        Yes|          Yes|       Auto|        Yes|      0|       121173|                    0|                    1|              23.17|             60|           0.67|            1.0|                 1.0|                0.0|              0.0|                0.0|              4.0|              0.0|(3,[1],[1.0])|     (3,[1],[1.0])|    (2,[0],[1.0])|  (1,[0],[1.0])|    (1,[0],[1.0])|      (4,[],[])|  (1,[0],[1.0])|[35.0,121173.0,20...|[-0.5669219294993...|\n",
      "|LNLSA61H9Q| 48|104529|     48075|        767|             2|             4|        4.92|      24|    0.74|   Master's|     Part-time|      Married|        Yes|          Yes|  Education|        Yes|      0|       104529|                    2|                    4|               4.92|             24|           0.74|            2.0|                 0.0|                0.0|              0.0|                0.0|              2.0|              0.0|(3,[2],[1.0])|     (3,[0],[1.0])|    (2,[0],[1.0])|  (1,[0],[1.0])|    (1,[0],[1.0])|  (4,[2],[1.0])|  (1,[0],[1.0])|[48.0,104529.0,48...|[0.30030794985844...|\n",
      "|3D0HETFVI6| 64| 69695|     59430|        646|            83|             2|       14.47|      48|    0.34| Bachelor's|     Full-time|      Married|         No|           No|       Auto|        Yes|      0|        69695|                   83|                    2|              14.47|             48|           0.34|            0.0|                 3.0|                0.0|              1.0|                1.0|              4.0|              0.0|(3,[0],[1.0])|         (3,[],[])|    (2,[0],[1.0])|      (1,[],[])|        (1,[],[])|      (4,[],[])|  (1,[0],[1.0])|[64.0,69695.0,594...|[1.36766780137576...|\n",
      "|B712U8QUQ9| 19|106039|    142951|        561|            67|             2|       20.05|      60|    0.63|   Master's|    Unemployed|      Married|         No|          Yes|  Education|        Yes|      0|       106039|                   67|                    2|              20.05|             60|           0.63|            2.0|                 1.0|                0.0|              1.0|                0.0|              2.0|              0.0|(3,[2],[1.0])|     (3,[1],[1.0])|    (2,[0],[1.0])|      (1,[],[])|    (1,[0],[1.0])|  (4,[2],[1.0])|  (1,[0],[1.0])|[19.0,106039.0,14...|[-1.6342817810166...|\n",
      "|7Q88O1GE0G| 58| 42779|     54049|        606|            65|             2|        15.8|      12|    0.76|High School|     Full-time|     Divorced|         No|          Yes|      Other|         No|      0|        42779|                   65|                    2|               15.8|             12|           0.76|            1.0|                 3.0|                1.0|              1.0|                0.0|              3.0|              1.0|(3,[1],[1.0])|         (3,[],[])|    (2,[1],[1.0])|      (1,[],[])|    (1,[0],[1.0])|  (4,[3],[1.0])|      (1,[],[])|[58.0,42779.0,540...|[0.96740785705676...|\n",
      "|23HB3DMK4A| 37|135671|     48451|        576|            32|             1|         2.9|      60|    0.64|   Master's|    Unemployed|      Married|         No|           No|      Other|         No|      1|       135671|                   32|                    1|                2.9|             60|           0.64|            2.0|                 1.0|                0.0|              1.0|                1.0|              3.0|              1.0|(3,[2],[1.0])|     (3,[1],[1.0])|    (2,[0],[1.0])|      (1,[],[])|        (1,[],[])|  (4,[3],[1.0])|      (1,[],[])|[37.0,135671.0,48...|[-0.4335019480597...|\n",
      "|O5O1CP51BP| 33| 57298|    169734|        647|            32|             3|        7.43|      60|    0.72|   Master's|     Part-time|      Married|         No|          Yes|   Business|         No|      0|        57298|                   32|                    3|               7.43|             60|           0.72|            2.0|                 0.0|                0.0|              1.0|                0.0|              0.0|              1.0|(3,[2],[1.0])|     (3,[0],[1.0])|    (2,[0],[1.0])|      (1,[],[])|    (1,[0],[1.0])|  (4,[0],[1.0])|      (1,[],[])|[33.0,57298.0,169...|[-0.7003419109390...|\n",
      "|YCKGYY5D3N| 56|125861|     62755|        550|            29|             2|       22.18|      12|    0.18|        PhD|     Full-time|       Single|         No|           No|      Other|         No|      1|       125861|                   29|                    2|              22.18|             12|           0.18|            3.0|                 3.0|                2.0|              1.0|                1.0|              3.0|              1.0|    (3,[],[])|         (3,[],[])|        (2,[],[])|      (1,[],[])|        (1,[],[])|  (4,[3],[1.0])|      (1,[],[])|[56.0,125861.0,62...|[0.83398787561710...|\n",
      "|8SGQPYYZDJ| 39| 90980|     48014|        508|            65|             1|       23.47|      48|    0.36|   Master's| Self-employed|       Single|        Yes|          Yes|       Home|        Yes|      0|        90980|                   65|                    1|              23.47|             48|           0.36|            2.0|                 2.0|                2.0|              0.0|                0.0|              1.0|              0.0|(3,[2],[1.0])|     (3,[2],[1.0])|        (2,[],[])|  (1,[0],[1.0])|    (1,[0],[1.0])|  (4,[1],[1.0])|  (1,[0],[1.0])|[39.0,90980.0,480...|[-0.3000819666200...|\n",
      "|C1ZRTUW8IW| 63| 16198|     42996|        405|           106|             3|        23.5|      24|     0.7|   Master's| Self-employed|       Single|         No|           No|       Auto|        Yes|      0|        16198|                  106|                    3|               23.5|             24|            0.7|            2.0|                 2.0|                2.0|              1.0|                1.0|              4.0|              0.0|(3,[2],[1.0])|     (3,[2],[1.0])|        (2,[],[])|      (1,[],[])|        (1,[],[])|      (4,[],[])|  (1,[0],[1.0])|[63.0,16198.0,429...|[1.30095781065592...|\n",
      "|FQCG7SQCJN| 62|105894|    123693|        439|            27|             3|        2.47|      36|    0.25|        PhD|     Full-time|     Divorced|         No|           No|       Auto|        Yes|      0|       105894|                   27|                    3|               2.47|             36|           0.25|            3.0|                 3.0|                1.0|              1.0|                1.0|              4.0|              0.0|    (3,[],[])|         (3,[],[])|    (2,[1],[1.0])|      (1,[],[])|        (1,[],[])|      (4,[],[])|  (1,[0],[1.0])|[62.0,105894.0,12...|[1.23424781993609...|\n",
      "|Q8BZ3B8A4G| 50|119807|    117375|        552|            88|             3|       22.48|      60|    0.22| Bachelor's|    Unemployed|       Single|        Yes|           No|       Home|         No|      0|       119807|                   88|                    3|              22.48|             60|           0.22|            0.0|                 1.0|                2.0|              0.0|                1.0|              1.0|              1.0|(3,[0],[1.0])|     (3,[1],[1.0])|        (2,[],[])|  (1,[0],[1.0])|        (1,[],[])|  (4,[1],[1.0])|      (1,[],[])|[50.0,119807.0,11...|[0.43372793129811...|\n",
      "|RKFHCA9JQE| 63|101556|     71258|        508|            65|             3|       11.39|      12|    0.21|   Master's|    Unemployed|       Single|         No|          Yes|       Home|        Yes|      0|       101556|                   65|                    3|              11.39|             12|           0.21|            2.0|                 1.0|                2.0|              1.0|                0.0|              1.0|              0.0|(3,[2],[1.0])|     (3,[1],[1.0])|        (2,[],[])|      (1,[],[])|    (1,[0],[1.0])|  (4,[1],[1.0])|  (1,[0],[1.0])|[63.0,101556.0,71...|[1.30095781065592...|\n",
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+-------------+---------------------+---------------------+-------------------+---------------+---------------+---------------+--------------------+-------------------+-----------------+-------------------+-----------------+-----------------+-------------+------------------+-----------------+---------------+-----------------+---------------+---------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - ROC AUC: 0.7336549215886313\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"Default\", maxIter=10)\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Default\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "roc_auc_lr = evaluator.evaluate(lr_predictions)\n",
    "print(f\"Logistic Regression - ROC AUC: {roc_auc_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path /home/drissdo/Desktop/Scalable-Distributed-Systems/model already exists. Consider removing it or choosing a new path.\n"
     ]
    }
   ],
   "source": [
    "# Save the Logistic Regression model\n",
    "model_path = \"/home/drissdo/Desktop/Scalable-Distributed-Systems/model\"\n",
    "\n",
    "# Check if the path already exists and handle it (e.g., overwrite or raise an error)\n",
    "import os\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Path {model_path} already exists. Consider removing it or choosing a new path.\")\n",
    "\n",
    "# Save the model\n",
    "lr_model.write().overwrite().save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - ROC AUC: 0.7336530388666921\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "# Load the saved model\n",
    "loadedModel = LogisticRegressionModel.load(model_path)\n",
    "\n",
    "# Verify by making predictions using the loaded model\n",
    "lr_predictions = loadedModel.transform(test_data)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Default\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "roc_auc_lr = evaluator.evaluate(lr_predictions)\n",
    "print(f\"Logistic Regression - ROC AUC: {roc_auc_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 188:===================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - ROC AUC: 0.4210816058940954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol=\"scaled_features\", labelCol=\"Default\", maxDepth=10)\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_dt = evaluator.evaluate(dt_predictions)\n",
    "print(f\"Decision Tree - ROC AUC: {roc_auc_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 243:===================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - ROC AUC: 0.7071944814368916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=\"Default\", numTrees=100, maxDepth=5)\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_rf = evaluator.evaluate(rf_predictions)\n",
    "print(f\"Random Forest - ROC AUC: {roc_auc_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 424:===================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient-Boosted Trees - ROC AUC: 0.7311417017458507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(featuresCol=\"scaled_features\", labelCol=\"Default\", maxIter=10, maxDepth=5)\n",
    "gbt_model = gbt.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_gbt = evaluator.evaluate(gbt_predictions)\n",
    "print(f\"Gradient-Boosted Trees - ROC AUC: {roc_auc_gbt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy: 0.8850444687158683, Precision: 0.8498232051102164, Recall: 0.8850444687158684, F1 Score: 0.8354146891676907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Accuracy: 0.8822554220627243, Precision: 0.8370298576990229, Recall: 0.8822554220627243, F1 Score: 0.841067195133172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Accuracy: 0.8844008425651427, Precision: 0.7821648503299344, Recall: 0.8844008425651427, F1 Score: 0.8301469970319177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 532:===================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient-Boosted Trees - Accuracy: 0.8852980184116087, Precision: 0.8498310312720517, Recall: 0.8852980184116087, F1 Score: 0.8382626093184488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Define evaluators\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"Default\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"Default\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"Default\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"Default\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "accuracy_lr = accuracy_evaluator.evaluate(lr_predictions)\n",
    "precision_lr = precision_evaluator.evaluate(lr_predictions)\n",
    "recall_lr = recall_evaluator.evaluate(lr_predictions)\n",
    "f1_lr = f1_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"Logistic Regression - Accuracy: {accuracy_lr}, Precision: {precision_lr}, Recall: {recall_lr}, F1 Score: {f1_lr}\")\n",
    "\n",
    "# Evaluate Decision Tree\n",
    "accuracy_dt = accuracy_evaluator.evaluate(dt_predictions)\n",
    "precision_dt = precision_evaluator.evaluate(dt_predictions)\n",
    "recall_dt = recall_evaluator.evaluate(dt_predictions)\n",
    "f1_dt = f1_evaluator.evaluate(dt_predictions)\n",
    "\n",
    "print(f\"Decision Tree - Accuracy: {accuracy_dt}, Precision: {precision_dt}, Recall: {recall_dt}, F1 Score: {f1_dt}\")\n",
    "\n",
    "# Evaluate Random Forest\n",
    "accuracy_rf = accuracy_evaluator.evaluate(rf_predictions)\n",
    "precision_rf = precision_evaluator.evaluate(rf_predictions)\n",
    "recall_rf = recall_evaluator.evaluate(rf_predictions)\n",
    "f1_rf = f1_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Random Forest - Accuracy: {accuracy_rf}, Precision: {precision_rf}, Recall: {recall_rf}, F1 Score: {f1_rf}\")\n",
    "\n",
    "# Evaluate Gradient-Boosted Trees\n",
    "accuracy_gbt = accuracy_evaluator.evaluate(gbt_predictions)\n",
    "precision_gbt = precision_evaluator.evaluate(gbt_predictions)\n",
    "recall_gbt = recall_evaluator.evaluate(gbt_predictions)\n",
    "f1_gbt = f1_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "print(f\"Gradient-Boosted Trees - Accuracy: {accuracy_gbt}, Precision: {precision_gbt}, Recall: {recall_gbt}, F1 Score: {f1_gbt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validated Model - ROC AUC: 0.7380937269287152\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# # Define parameter grid\n",
    "# paramGrid = ParamGridBuilder() \\\n",
    "#     .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "#     .addGrid(dt.maxDepth, [3, 5]) \\\n",
    "#     .addGrid(rf.numTrees, [50, 100]) \\\n",
    "#     .addGrid(gbt.maxIter, [5, 10]) \\\n",
    "#     .build()\n",
    "\n",
    "# # Define cross-validator\n",
    "# crossval = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# # Fit the model\n",
    "# cvModel = crossval.fit(train_data)\n",
    "\n",
    "# # Make predictions\n",
    "# cv_predictions = cvModel.transform(test_data)\n",
    "\n",
    "# # Evaluate the model\n",
    "# roc_auc_cv = evaluator.evaluate(cv_predictions)\n",
    "# print(f\"Cross-Validated Model - ROC AUC: {roc_auc_cv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
