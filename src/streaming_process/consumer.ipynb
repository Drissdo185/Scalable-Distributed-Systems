{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LoanApplicationPredictor\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.238.34.227:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>LoanApplicationPredictor</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7866c2707990>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"LoanID\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Income\", IntegerType(), True),\n",
    "    StructField(\"LoanAmount\", IntegerType(), True),\n",
    "    StructField(\"CreditScore\", IntegerType(), True),\n",
    "    StructField(\"MonthsEmployed\", IntegerType(), True),\n",
    "    StructField(\"NumCreditLines\", IntegerType(), True),\n",
    "    StructField(\"InterestRate\", DoubleType(), True),\n",
    "    StructField(\"LoanTerm\", IntegerType(), True),\n",
    "    StructField(\"DTIRatio\", DoubleType(), True),\n",
    "    StructField(\"Education\", StringType(), True),\n",
    "    StructField(\"EmploymentType\", StringType(), True),\n",
    "    StructField(\"MaritalStatus\", StringType(), True),\n",
    "    StructField(\"HasMortgage\", StringType(), True),\n",
    "    StructField(\"HasDependents\", StringType(), True),\n",
    "    StructField(\"LoanPurpose\", StringType(), True),\n",
    "    StructField(\"HasCoSigner\", StringType(), True),\n",
    "    StructField(\"Default\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    'loan_applications',\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "model_path = \"/home/drissdo/Desktop/Scalable-Distributed-Systems/ML/model\"\n",
    "lr_model = LogisticRegressionModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(loan_data):\n",
    "    imputer = Imputer(\n",
    "    inputCols=[\"Income\", \"MonthsEmployed\", \"NumCreditLines\", \"InterestRate\", \"LoanTerm\", \"DTIRatio\"],\n",
    "    outputCols=[\"Income_filled\", \"MonthsEmployed_filled\", \"NumCreditLines_filled\", \"InterestRate_filled\", \"LoanTerm_filled\", \"DTIRatio_filled\"]\n",
    ")\n",
    "    loan_data_imputed = imputer.fit(loan_data).transform(loan_data)\n",
    "\n",
    "    string_indexers = [\n",
    "        StringIndexer(inputCol=\"Education\", outputCol=\"Education_index\"),\n",
    "        StringIndexer(inputCol=\"EmploymentType\", outputCol=\"EmploymentType_index\"),\n",
    "        StringIndexer(inputCol=\"MaritalStatus\", outputCol=\"MaritalStatus_index\"),\n",
    "        StringIndexer(inputCol=\"HasMortgage\", outputCol=\"HasMortgage_index\"),\n",
    "        StringIndexer(inputCol=\"HasDependents\", outputCol=\"HasDependents_index\"),\n",
    "        StringIndexer(inputCol=\"LoanPurpose\", outputCol=\"LoanPurpose_index\"),\n",
    "        StringIndexer(inputCol=\"HasCoSigner\", outputCol=\"HasCoSigner_index\")\n",
    "    ]\n",
    "\n",
    "    pipeline_indexers = Pipeline(stages=string_indexers)\n",
    "    loan_data_indexed = pipeline_indexers.fit(loan_data_imputed).transform(loan_data_imputed)\n",
    "    \n",
    "    one_hot_encoders = [\n",
    "    OneHotEncoder(inputCol=\"Education_index\", outputCol=\"Education_vec\"),\n",
    "    OneHotEncoder(inputCol=\"EmploymentType_index\", outputCol=\"EmploymentType_vec\"),\n",
    "    OneHotEncoder(inputCol=\"MaritalStatus_index\", outputCol=\"MaritalStatus_vec\"),\n",
    "    OneHotEncoder(inputCol=\"HasMortgage_index\", outputCol=\"HasMortgage_vec\"),\n",
    "    OneHotEncoder(inputCol=\"HasDependents_index\", outputCol=\"HasDependents_vec\"),\n",
    "    OneHotEncoder(inputCol=\"LoanPurpose_index\", outputCol=\"LoanPurpose_vec\"),\n",
    "    OneHotEncoder(inputCol=\"HasCoSigner_index\", outputCol=\"HasCoSigner_vec\")\n",
    "]\n",
    "\n",
    "    pipeline_encoders = Pipeline(stages=one_hot_encoders)\n",
    "    loan_data_encoded = pipeline_encoders.fit(loan_data_indexed).transform(loan_data_indexed)\n",
    "\n",
    "    # Normalize numerical features\n",
    "    numerical_cols = [\n",
    "        \"Age\", \"Income_filled\", \"LoanAmount\", \"CreditScore\", \"MonthsEmployed_filled\",\n",
    "        \"NumCreditLines_filled\", \"InterestRate_filled\", \"LoanTerm_filled\", \"DTIRatio_filled\"\n",
    "    ]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"features\")\n",
    "    loan_data_assembled = assembler.transform(loan_data_encoded)\n",
    "\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "    scaler_model = scaler.fit(loan_data_assembled)\n",
    "    loan_data_scaled = scaler_model.transform(loan_data_assembled)\n",
    "\n",
    "    loan_data_scaled = loan_data_scaled.select([\"LoanID\", \"scaled_features\"])\n",
    "\n",
    "    return loan_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected message: Q47SC11GPK\n",
      "Collected message: 5PKRDS9T6G\n",
      "Collected message: DT0Q970YGH\n",
      "Collected message: A38RC479WR\n",
      "Collected message: CRDXTPI8KY\n",
      "Collected message: V5L1T406JF\n",
      "Collected message: P3EG7LBMIC\n",
      "Collected message: 0HCI1B2YTB\n",
      "Collected message: BV6I61ZH8K\n",
      "Collected message: 8LX0LK8Y7J\n",
      "Collected message: UMJ25ZD6PX\n",
      "Collected message: VHVCKK7F25\n",
      "Collected message: XN2FOKJ26P\n",
      "Collected message: BB9FDTKPFE\n",
      "Collected message: 0HOJTGKRZS\n",
      "Collected message: 6XW7VHV3DZ\n",
      "Collected message: 3AU7U4IKEO\n",
      "Collected message: EXENZ2QVJX\n",
      "Collected message: 49BYIQJMSN\n",
      "Collected message: A6HFUDU3NH\n",
      "Collected message: T8SHS85JJW\n",
      "Collected message: ANV7HYZS6S\n",
      "Collected message: CH31F65HON\n",
      "Collected message: L2I6M1JXAP\n",
      "Collected message: KBZ11MLLJ9\n",
      "Collected message: H2XT0LWT35\n",
      "Collected message: IGGM3Z04E7\n",
      "Collected message: QVA7VQCAEJ\n",
      "Collected message: 613VQWAH7P\n",
      "Collected message: 565JDAH6BC\n",
      "Collected message: ZS4Z66KNPF\n",
      "Collected message: 43FR8OTJX2\n",
      "Collected message: 1JLC1M0V6G\n",
      "Collected message: GIJ2SG5KDJ\n",
      "Collected message: X0JHSKSNFZ\n",
      "Collected message: LIUI1Q47XW\n",
      "Collected message: K8OSQFMLCY\n",
      "Collected message: O04CE10ZT8\n",
      "Collected message: 1160ARVADX\n",
      "Collected message: 1NL2WGJV2T\n",
      "Collected message: 8UEGNS69TJ\n",
      "Collected message: E4QUM6QUA3\n",
      "Collected message: RDSVPHKJIZ\n",
      "Collected message: GUGM7W13HH\n",
      "Collected message: I3Q6DUIQR6\n",
      "Collected message: 553H0DVS85\n",
      "Collected message: DFKJWF28TI\n",
      "Collected message: 5HFEKC5GX9\n",
      "Collected message: QLV5EH54T3\n",
      "Collected message: AL6F1SSX25\n",
      "Collected message: UI47HT3I5H\n",
      "Collected message: LC3TSPWTB7\n",
      "Collected message: HPHZMLVNBN\n",
      "Collected message: Y1E4PG5KIM\n",
      "Collected message: KZB384G9E4\n",
      "Collected message: W91HTG0XPA\n",
      "Collected message: 79N8BAOQJV\n",
      "Collected message: NNM75631JQ\n",
      "Collected message: Z7IMM8G275\n",
      "Collected message: 1GXRF9M846\n",
      "Collected message: 0U0M8EP239\n",
      "Collected message: IGDS20ZRE9\n",
      "Collected message: NC43IQJBQC\n",
      "Collected message: D0B84ZAL2V\n",
      "Collected message: 24SFO61UA7\n",
      "Collected message: DFD1LHPXPU\n",
      "Collected message: WLHDA7W3PV\n",
      "Collected message: 3IWXYY5NCJ\n",
      "Collected message: 61R9T5QU25\n",
      "Collected message: P2RQ8N8OUX\n",
      "Collected message: KGLH2XXFJV\n",
      "Collected message: HKINFMY99F\n",
      "Collected message: UQC50R5F01\n",
      "Collected message: CMSVZ4WU4K\n",
      "Collected message: S227SNGBBR\n",
      "Collected message: EH3S0EP1EF\n",
      "Collected message: QP05L8QAMD\n",
      "Collected message: 7YJGVI5OD4\n",
      "Collected message: AIUIPNXTQO\n",
      "Collected message: U3IHRIQKJ2\n",
      "Collected message: 3DSC1VQ5FV\n",
      "Collected message: AUE3P5X48V\n",
      "Collected message: CLXG3M5TDV\n",
      "Collected message: MZ758BXK8V\n",
      "Collected message: 6S2YVMPHVN\n",
      "Collected message: PZQZQLGOM9\n",
      "Collected message: 4SZCPYEXB8\n",
      "Collected message: PES255MKUB\n",
      "Collected message: PAVNOKK85A\n",
      "Collected message: 7J38UB58XA\n",
      "Collected message: J5FYIRMEL6\n",
      "Collected message: 1KYSOPPH6A\n",
      "Collected message: G7AQGS9Z5U\n",
      "Collected message: RGFGRJGXJD\n",
      "Collected message: 44BSYMAEUO\n",
      "Collected message: DBXFWVRC8U\n",
      "Collected message: JUY6S9FKHX\n",
      "Collected message: IE6OL44J7I\n",
      "Collected message: EDL0C9YB2S\n",
      "Error collecting messages: 'LoanID'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records collected: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions:\n",
      "+----------+----------+\n",
      "|    LoanID|prediction|\n",
      "+----------+----------+\n",
      "|Q47SC11GPK|       0.0|\n",
      "|5PKRDS9T6G|       0.0|\n",
      "|DT0Q970YGH|       0.0|\n",
      "|A38RC479WR|       0.0|\n",
      "|CRDXTPI8KY|       0.0|\n",
      "|V5L1T406JF|       0.0|\n",
      "|P3EG7LBMIC|       0.0|\n",
      "|0HCI1B2YTB|       0.0|\n",
      "|BV6I61ZH8K|       0.0|\n",
      "|8LX0LK8Y7J|       0.0|\n",
      "|UMJ25ZD6PX|       0.0|\n",
      "|VHVCKK7F25|       0.0|\n",
      "|XN2FOKJ26P|       0.0|\n",
      "|BB9FDTKPFE|       0.0|\n",
      "|0HOJTGKRZS|       0.0|\n",
      "|6XW7VHV3DZ|       0.0|\n",
      "|3AU7U4IKEO|       0.0|\n",
      "|EXENZ2QVJX|       0.0|\n",
      "|49BYIQJMSN|       0.0|\n",
      "|A6HFUDU3NH|       0.0|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 10:28:49 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Requires group_id",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid messages collected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Optional: Commit offsets after processing\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mconsumer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Scalable-Distributed-Systems/venv/lib/python3.11/site-packages/kafka/consumer/group.py:524\u001b[0m, in \u001b[0;36mKafkaConsumer.commit\u001b[0;34m(self, offsets)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Commit offsets to kafka, blocking until success or error.\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \n\u001b[1;32m    506\u001b[0m \u001b[38;5;124;03mThis commits offsets only to Kafka. The offsets committed using this API\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;124;03m        consumed offsets for all subscribed partitions.\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRequires >= Kafka 0.8.1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 524\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRequires group_id\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offsets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    526\u001b[0m     offsets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscription\u001b[38;5;241m.\u001b[39mall_consumed_offsets()\n",
      "\u001b[0;31mAssertionError\u001b[0m: Requires group_id"
     ]
    }
   ],
   "source": [
    "# Create empty list to store all messages\n",
    "messages_list = []\n",
    "\n",
    "# 1. Collect all messages from consumer\n",
    "try:\n",
    "    for message in consumer:\n",
    "        if isinstance(message.value, dict):\n",
    "            messages_list.append(message.value)\n",
    "            print(f\"Collected message: {message.value['LoanID']}\")  # Log collection\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error collecting messages: {e}\")\n",
    "\n",
    "# 2. Process all collected messages if we have any\n",
    "if messages_list:\n",
    "    try:\n",
    "        # Create DataFrame from all collected messages\n",
    "        df = spark.createDataFrame(messages_list, schema=schema)\n",
    "        print(f\"Total records collected: {df.count()}\")\n",
    "        \n",
    "        # Process all data at once\n",
    "        test_data = preprocess_data(df)\n",
    "        lr_predictions = lr_model.transform(test_data)\n",
    "        \n",
    "        # Show predictions\n",
    "        print(\"\\nPredictions:\")\n",
    "        lr_predictions.select(\"LoanID\", \"prediction\").show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {e}\")\n",
    "else:\n",
    "    print(\"No valid messages collected\")\n",
    "\n",
    "# Optional: Commit offsets after processing\n",
    "consumer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
